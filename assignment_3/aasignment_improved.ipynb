{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3c1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import music21 as m21\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # cuz my version of python complains about deprecated stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f500e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_path = \"songs\"\n",
    "acceptable_durations = [0.25, 0.5, 0.75, 1.0, 1.5, 2, 3, 4]\n",
    "preprocessing_path = \"preprocessing\"\n",
    "single_file_path = \"input\"\n",
    "mapping_path = \"mapping.json\"\n",
    "model_path = \"model.h5\"\n",
    "sequence_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a3cb7",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950b8b7",
   "metadata": {},
   "source": [
    "This is the hardest part by far. I have some experience with music theory, which kinda helped.\n",
    "I made my own training data inspired by snippets of the lofi songs. These followed a strict format, using only single notes or major triad chords.\n",
    "Once these were made, I transposed them to C major or A minor (to avoid sharps/flats) and encoded the songs using a custom mapping that only contains the single notes or triads. The mappings follow the MIDI numbers for each note. Chords are separated by periods. Rests and measure breaks also have their own symbol to establish rhythm during encoding.\n",
    "\n",
    "    \"48.52.55\": 1, # C major\n",
    "    \"50.53.57\": 2, # D minor\n",
    "    \"52.55.59\": 3, # E minor\n",
    "    \"53.57.60\": 4, # F major\n",
    "    \"55.59.62\": 5, # G major\n",
    "    \"57.60.64\": 6, # A minor\n",
    "    \"59.62.65\": 7, # B diminished\n",
    "    \"60.64.67\": 8, # C major (higher octave)\n",
    "    \"62.65.69\": 9, # D minor (higher octave)\n",
    "    \"64.67.71\": 10, # E minor (higher octave)\n",
    "    \"65.69.72\": 11, # F major (higher octave)\n",
    "    \"67.71.74\": 12, # G major (higher octave)\n",
    "    \"69.72.76\": 13, # A minor (higher octave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c3df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(folder_path):\n",
    "    songs = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        song = m21.converter.parse(os.path.join(folder_path, file))\n",
    "        songs.append(song)\n",
    "\n",
    "    return songs\n",
    "\n",
    "def has_acceptable_notes(song, acceptable_durations):\n",
    "    for note in song.flatten().notesAndRests: # filters out metadata from the m21 stuff so it's just the notes\n",
    "        if note.duration.quarterLength not in acceptable_durations:\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "def transpose(song):\n",
    "    key = song.analyze('key')\n",
    "\n",
    "    if key.mode == 'major':\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch('C'))\n",
    "    elif key.mode == 'minor':\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch('A'))\n",
    "\n",
    "    transposed_song = song.transpose(interval)\n",
    "    return transposed_song\n",
    "\n",
    "def encode_song(song, time_step = 0.25):\n",
    "\n",
    "    encoded_song = []\n",
    "\n",
    "    for note in song.flatten().notesAndRests:\n",
    "        if isinstance(note, m21.note.Note):\n",
    "            symbol = f\"{note.pitch.midi}\"\n",
    "        elif isinstance(note, m21.note.Rest):\n",
    "            symbol = \"r\"\n",
    "        elif isinstance(note, m21.chord.Chord):\n",
    "            symbol = \".\".join(str(n.midi) for n in note.pitches)\n",
    "        \n",
    "        steps = int(note.duration.quarterLength / time_step)\n",
    "\n",
    "        for step in range(steps):\n",
    "            if step == 0:\n",
    "                encoded_song.append(symbol)\n",
    "            else:\n",
    "                encoded_song.append(\"_\")\n",
    "\n",
    "    encoded_song = \" \".join(map(str, encoded_song))\n",
    "    return encoded_song\n",
    "\n",
    "def load(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        song = f.read()\n",
    "    return song\n",
    "\n",
    "def condense(dataset_path, file_dataset_path, sequence_length=64):\n",
    "    delimiter = \"/ \" * sequence_length\n",
    "    songs = \"\"\n",
    "\n",
    "    for file in os.listdir(dataset_path):\n",
    "        path = os.path.join(dataset_path, file)\n",
    "        song = load(path)\n",
    "        songs += song + \" \" + delimiter\n",
    "    songs = songs[:-1] # gets rid of the final delimiter\n",
    "\n",
    "    with open(file_dataset_path, \"w\") as f:\n",
    "        f.write(songs)\n",
    "\n",
    "    return songs\n",
    "\n",
    "\n",
    "def preprocess(folder_path):\n",
    "    # load everything\n",
    "    print(\"Loading songs...\")\n",
    "    songs = load_songs(folder_path)\n",
    "    print(f\"Loaded {len(songs)} songs.\\n\")\n",
    "    \n",
    "    for i, song in enumerate(songs):\n",
    "        # filter out songs that have weird notes and duration\n",
    "        if not has_acceptable_notes(song, acceptable_durations):\n",
    "            continue\n",
    "\n",
    "        # transpose to either C major or A minor, since both have no flats/sharps and you need both major/minor for mood\n",
    "        transposed_song = transpose(song)\n",
    "\n",
    "        # encode to time series\n",
    "        encoded_song = encode_song(transposed_song)\n",
    "\n",
    "        save_path = os.path.join(preprocessing_path, str(i))\n",
    "        with open(save_path + \".txt\", \"w\") as f:\n",
    "            f.write(encoded_song)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8428f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading songs...\n",
      "Loaded 93 songs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess(songs_path)\n",
    "songs = condense(preprocessing_path, single_file_path, sequence_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f818e6",
   "metadata": {},
   "source": [
    "Mapping to Readable Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff74ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(songs, mapping_path):\n",
    "    mappings = {}\n",
    "\n",
    "    songs = songs.split()\n",
    "    vocab = sorted(set(songs)) # keeps unique symbols in order\n",
    "\n",
    "    for i, symbol in enumerate(vocab): # isolates unique symbols\n",
    "        mappings[symbol] = i\n",
    "\n",
    "    with open(mapping_path, \"w\") as f:\n",
    "        json.dump(mappings, f, indent=4)\n",
    "\n",
    "def convert_to_int(songs):\n",
    "    int_songs = []\n",
    "\n",
    "    with open(mapping_path, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    songs = songs.split()\n",
    "\n",
    "    for symbol in songs:\n",
    "        int_songs.append(mappings[symbol])\n",
    "\n",
    "    return int_songs\n",
    "\n",
    "def generate_batches(sequence_length):\n",
    "    songs = load(single_file_path)\n",
    "    int_songs = convert_to_int(songs)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    num_sequences = len(int_songs) - sequence_length\n",
    "    for i in range(num_sequences):\n",
    "        inputs.append(int_songs[i:i + sequence_length]) # slides time series by one step\n",
    "        targets.append(int_songs[i + sequence_length])\n",
    "\n",
    "    vocab_size = len(set(int_songs))\n",
    "    inputs = keras.utils.to_categorical(inputs, num_classes=vocab_size)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b162224",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mapping(songs, mapping_path)\n",
    "inputs, targets = generate_batches(sequence_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab040ffe",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4837cd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">130,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,254</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m130,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m)            │        \u001b[38;5;34m16,254\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">146,814</span> (573.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m146,814\u001b[0m (573.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">146,814</span> (573.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m146,814\u001b[0m (573.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 126\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, vocab_size)))\n",
    "model.add(keras.layers.LSTM(128))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=keras.optimizers.Nadam(), metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "180931ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.6365 - loss: 2.1939\n",
      "Epoch 2/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7876 - loss: 1.1620\n",
      "Epoch 3/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8056 - loss: 1.0837\n",
      "Epoch 4/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8122 - loss: 1.0197\n",
      "Epoch 5/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8240 - loss: 0.9579\n",
      "Epoch 6/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8259 - loss: 0.9510\n",
      "Epoch 7/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8253 - loss: 0.9382\n",
      "Epoch 8/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8292 - loss: 0.9018\n",
      "Epoch 9/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8282 - loss: 0.8897\n",
      "Epoch 10/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8341 - loss: 0.8708\n",
      "Epoch 11/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8380 - loss: 0.8492\n",
      "Epoch 12/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8361 - loss: 0.8447\n",
      "Epoch 13/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8380 - loss: 0.8282\n",
      "Epoch 14/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8370 - loss: 0.8154\n",
      "Epoch 15/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8380 - loss: 0.8135\n",
      "Epoch 16/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8164 - loss: 0.9242\n",
      "Epoch 17/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8321 - loss: 0.8542\n",
      "Epoch 18/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8387 - loss: 0.8029\n",
      "Epoch 19/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8439 - loss: 0.7779\n",
      "Epoch 20/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8442 - loss: 0.7746\n",
      "Epoch 21/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8462 - loss: 0.7576\n",
      "Epoch 22/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8511 - loss: 0.7397\n",
      "Epoch 23/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8531 - loss: 0.7295\n",
      "Epoch 24/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8547 - loss: 0.7195\n",
      "Epoch 25/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8590 - loss: 0.7055\n",
      "Epoch 26/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8573 - loss: 0.7009\n",
      "Epoch 27/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8635 - loss: 0.6868\n",
      "Epoch 28/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8606 - loss: 0.6736\n",
      "Epoch 29/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8619 - loss: 0.6670\n",
      "Epoch 30/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8593 - loss: 0.6660\n",
      "Epoch 31/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8632 - loss: 0.6539\n",
      "Epoch 32/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8645 - loss: 0.6538\n",
      "Epoch 33/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8711 - loss: 0.6327\n",
      "Epoch 34/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8698 - loss: 0.6264\n",
      "Epoch 35/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8737 - loss: 0.6109\n",
      "Epoch 36/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8717 - loss: 0.6097\n",
      "Epoch 37/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8730 - loss: 0.5913\n",
      "Epoch 38/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8714 - loss: 0.5938\n",
      "Epoch 39/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8766 - loss: 0.5763\n",
      "Epoch 40/40\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8747 - loss: 0.5761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.fit(inputs, targets, epochs=40, batch_size=64)\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59171e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "generator = keras.models.load_model(model_path)\n",
    "mappings = json.load(open(mapping_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6146b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, mappings, sequence_length, seed, num_steps=500, temperature=1.0):\n",
    "    reverse_mapping = {v: k for k, v in mappings.items()}\n",
    "    vocab_size = len(mappings)\n",
    "\n",
    "    seed_int = [mappings[symbol] for symbol in seed.split()]\n",
    "    output = list(seed_int)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        seed_sequence = output[-sequence_length:]\n",
    "        onehot = keras.utils.to_categorical(seed_sequence, num_classes=vocab_size)\n",
    "        onehot = np.expand_dims(onehot, axis=0)\n",
    "\n",
    "        preds = model.predict(onehot, verbose=0)[0]\n",
    "\n",
    "        preds = np.log(preds + 1e-8) / temperature\n",
    "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "\n",
    "        next_idx = np.random.choice(range(vocab_size), p=preds)\n",
    "        output.append(next_idx)\n",
    "\n",
    "    generated = [reverse_mapping[i] for i in output]\n",
    "    return \" \".join(generated)\n",
    "\n",
    "def decode_song(encoded_song, time_step=0.25):\n",
    "\n",
    "    song_stream = m21.stream.Stream()\n",
    "    symbols = encoded_song.split()\n",
    "    step_counter = 0\n",
    "    prev_symbol = None\n",
    "\n",
    "    for symbol in symbols:\n",
    "        if symbol != \"_\":\n",
    "            if prev_symbol and step_counter > 0:\n",
    "                dur = m21.duration.Duration(step_counter * time_step)\n",
    "                song_stream[-1].duration = dur\n",
    "            if symbol == \"r\" or symbol == \"/\":\n",
    "                song_stream.append(m21.note.Rest())\n",
    "            elif \".\" in symbol:\n",
    "                pitches = [int(x) for x in symbol.split(\".\")]\n",
    "                song_stream.append(m21.chord.Chord(pitches))\n",
    "            else:\n",
    "                midi_number = int(symbol)\n",
    "                song_stream.append(m21.note.Note(midi_number))\n",
    "            step_counter = 1\n",
    "            prev_symbol = symbol\n",
    "        else:\n",
    "            step_counter += 1\n",
    "\n",
    "    song_stream.write(\"midi\", fp=\"generated_song.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e070efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"67 _ _ _ _ _ 69 _ _ 71 _ _ _ _ _ 72 _ _ _ _ 71 _ _ 69 _ _ _ _\"  # or take from your dataset\n",
    "generated = generate(model, mappings, sequence_length=64, seed=seed, num_steps=200, temperature=2)\n",
    "decode_song(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ea6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
